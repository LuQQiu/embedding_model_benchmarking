# Benchmark configuration
# Controls which model to test and benchmark scenarios

# Active model to benchmark (change this to test different models)
active_model: "embeddinggemma-300m"

# Test dataset configuration
dataset:
  # Use a subset of MS MARCO for testing
  name: "ms_marco"
  source: "sentence-transformers/embedding-training-data"
  subset: "msmarco-passage"
  num_samples: 10000  # Total samples to load
  sequence_lengths: [32, 128, 512]  # Test with different input lengths

  # Alternative: use custom sentences
  # custom_sentences:
  #   - "This is a test sentence for benchmarking."
  #   - "Another example sentence with different length and content."

# Warmup configuration
warmup:
  enabled: true
  num_requests: 100
  description: "Warmup phase to avoid cold start effects (JIT, cache warming)"

# Benchmark scenarios
scenarios:
  # Single query - baseline latency
  - name: "single_query"
    concurrency: 1
    num_requests: 1000
    batch_size: 1
    description: "Baseline single-threaded performance"

  # Low concurrency
  - name: "low_concurrency"
    concurrency: 4
    num_requests: 5000
    batch_size: 1
    description: "Low concurrent load (4 parallel requests)"

  # Medium concurrency
  - name: "medium_concurrency"
    concurrency: 16
    num_requests: 10000
    batch_size: 1
    description: "Medium concurrent load (16 parallel requests)"

  # High concurrency
  - name: "high_concurrency"
    concurrency: 64
    num_requests: 20000
    batch_size: 1
    description: "High concurrent load (64 parallel requests)"

  # Very high concurrency (stress test)
  - name: "stress_test"
    concurrency: 128
    num_requests: 30000
    batch_size: 1
    description: "Stress test with 128 concurrent requests"

  # Batch processing
  - name: "batch_processing"
    concurrency: 1
    num_requests: 500
    batch_size: 8
    description: "Batch processing with batch size 8"

  # Batch processing with concurrency
  - name: "batch_concurrent"
    concurrency: 8
    num_requests: 2000
    batch_size: 4
    description: "Concurrent batch processing (8 workers, batch size 4)"

# Metrics to collect
metrics:
  latency:
    - mean
    - median  # p50
    - p95
    - p99
    - p999
    - min
    - max
    - stddev

  throughput:
    - qps  # Queries per second
    - total_requests
    - duration_seconds

  resources:
    - cpu_percent
    - memory_rss_mb
    - memory_peak_mb

  model:
    - load_time_ms
    - first_inference_ms

# Output configuration
output:
  format: "json"
  save_individual_timings: false  # Set to true to save all request timings (large files)
  results_dir: "/results"
  filename_template: "{model_name}/{framework}.json"

# System monitoring
monitoring:
  enabled: true
  interval_seconds: 1
  metrics:
    - cpu_usage
    - memory_usage
    - disk_io
